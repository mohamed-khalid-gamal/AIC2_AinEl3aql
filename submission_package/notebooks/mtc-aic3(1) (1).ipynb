{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-29T07:40:08.386573Z",
     "iopub.status.busy": "2025-06-29T07:40:08.386248Z",
     "iopub.status.idle": "2025-06-29T07:45:19.991776Z",
     "shell.execute_reply": "2025-06-29T07:45:19.990988Z",
     "shell.execute_reply.started": "2025-06-29T07:40:08.386553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index files...\n",
      "  Train=2400, Val=50, Test=50\n",
      "  Epochs: train=2400, val=50, test=50\n",
      "[DeepConvNet] Epoch 1 val acc: 0.4800\n",
      "[DeepConvNet] Epoch 2 val acc: 0.4600\n",
      "[DeepConvNet] Epoch 3 val acc: 0.5000\n",
      "[DeepConvNet] Epoch 4 val acc: 0.4800\n",
      "[DeepConvNet] Epoch 5 val acc: 0.4000\n",
      "[DeepConvNet] Epoch 6 val acc: 0.4000\n",
      "[DeepConvNet] Epoch 7 val acc: 0.4600\n",
      "[DeepConvNet] Epoch 8 val acc: 0.5000\n",
      "[DeepConvNet] Epoch 9 val acc: 0.4000\n",
      "[DeepConvNet] Epoch 10 val acc: 0.3800\n",
      "[DeepConvNet] Epoch 11 val acc: 0.3800\n",
      "[DeepConvNet] Epoch 12 val acc: 0.4000\n",
      "[DeepConvNet] Epoch 13 val acc: 0.4000\n",
      "[DeepConvNet] Epoch 14 val acc: 0.4800\n",
      "[DeepConvNet] Epoch 15 val acc: 0.5400\n",
      "[DeepConvNet] Epoch 16 val acc: 0.4400\n",
      "[DeepConvNet] Epoch 17 val acc: 0.3800\n",
      "[DeepConvNet] Epoch 18 val acc: 0.4200\n",
      "[DeepConvNet] Epoch 19 val acc: 0.4200\n",
      "[DeepConvNet] Epoch 20 val acc: 0.4000\n",
      "Best val acc for DeepConvNet: 0.5400\n",
      "✅ Saved submission.csv\n",
      "✓ All test IDs covered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, stft, welch, hilbert\n",
    "from scipy.stats import linregress\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mne.decoding import CSP  # Requires mne-python package\n",
    "from mne.preprocessing import ICA as mne_ICA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.signal import gausspulse, chirp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pywt\n",
    "from scipy import stats\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add new imports for feature extraction\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.signal import spectrogram\n",
    "from pywt import wavedec\n",
    "\n",
    "\n",
    "def load_index_csvs_MI(base_path):\n",
    "    train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
    "    validation_df = pd.read_csv(os.path.join(base_path, 'validation.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
    "    \n",
    "    # Create separate label encoders for MI and SSVEP tasks\n",
    "    le_mi = LabelEncoder()\n",
    "    \n",
    "    # Fit encoders on training data only and transform all splits consistently\n",
    "    if 'label' in train_df.columns:\n",
    "        # MI task encoding\n",
    "        mi_train_labels = train_df[train_df['task'] == 'MI']['label']\n",
    "        if len(mi_train_labels) > 0:\n",
    "            le_mi.fit(mi_train_labels)\n",
    "            \n",
    "            # Transform MI labels in all splits\n",
    "            for df in [train_df, validation_df]:\n",
    "                if 'label' in df.columns:\n",
    "                    mi_mask = df['task'] == 'MI'\n",
    "                    if mi_mask.any():\n",
    "                        df.loc[mi_mask, 'label'] = le_mi.transform(df.loc[mi_mask, 'label'])\n",
    "        \n",
    "        \n",
    "\n",
    "    return train_df, validation_df, test_df, le_mi\n",
    "# Configuration\n",
    "SELECTED_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n",
    "SAMPLING_RATE = 250  # Hz (adjust if different)\n",
    "TRIAL_LENGTH = 2250  # Samples\n",
    "\n",
    "class EEGPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, filter_low=8, filter_high=30):\n",
    "        self.filter_low = filter_low\n",
    "        self.filter_high = filter_high\n",
    "        self.channel_indices = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: List of DataFrames (each from EEGdata.csv)\n",
    "        Returns: Normalized epochs (n_trials, 2250, 8)\n",
    "        \"\"\"\n",
    "        all_epochs = []\n",
    "        \n",
    "        for df in X:\n",
    "            # Step 1: Channel selection\n",
    "            eeg_data = df[SELECTED_CHANNELS].values\n",
    "            \n",
    "            # Step 2: Band-pass filter\n",
    "            nyquist = 0.5 * SAMPLING_RATE\n",
    "            low = self.filter_low / nyquist\n",
    "            high = self.filter_high / nyquist\n",
    "            b, a = butter(5, [low, high], btype='band')\n",
    "            filtered = filtfilt(b, a, eeg_data, axis=0)\n",
    "            \n",
    "            # Step 3: Epoch extraction\n",
    "            n_trials = len(df) // TRIAL_LENGTH\n",
    "            for i in range(n_trials):\n",
    "                start_idx = i * TRIAL_LENGTH\n",
    "                epoch = filtered[start_idx:start_idx + TRIAL_LENGTH]\n",
    "                \n",
    "                # Step 4: Per-channel normalization\n",
    "                normalized = (epoch - epoch.mean(axis=0)) / (epoch.std(axis=0) + 1e-8)\n",
    "                all_epochs.append(normalized)\n",
    "                \n",
    "        return np.array(all_epochs)\n",
    "\n",
    "# Feature Extraction Options\n",
    "class CSPFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=4):\n",
    "        self.csp = CSP(n_components=n_components, reg=None, log=True)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X shape: (n_trials, time_points, channels)\n",
    "        X_csp = X.transpose(0, 2, 1)  # MNE expects (trials, channels, time)\n",
    "        self.csp.fit(X_csp, y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_csp = X.transpose(0, 2, 1)\n",
    "        return self.csp.transform(X_csp)\n",
    "\n",
    "class STFTFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nperseg=250, noverlap=125):\n",
    "        self.nperseg = nperseg\n",
    "        self.noverlap = noverlap\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for trial in X:\n",
    "            # Compute power spectral density per channel\n",
    "            trial_features = []\n",
    "            for channel in range(trial.shape[1]):\n",
    "                f, t, Zxx = stft(trial[:, channel], \n",
    "                                fs=SAMPLING_RATE,\n",
    "                                nperseg=self.nperseg,\n",
    "                                noverlap=self.noverlap)\n",
    "                psd = np.abs(Zxx) ** 2\n",
    "                \n",
    "                # Extract alpha (8-12Hz) and beta (12-30Hz) bands\n",
    "                alpha = psd[(f >= 8) & (f < 12)].mean()\n",
    "                beta = psd[(f >= 12) & (f <= 30)].mean()\n",
    "                trial_features.extend([alpha, beta])\n",
    "                \n",
    "            features.append(trial_features)\n",
    "        return np.array(features)\n",
    "\n",
    "class RawEEGFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.reshape(X.shape[0], -1)  # Flatten trials\n",
    "    \n",
    "    \n",
    "class AutoRegressionFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for trial in X:\n",
    "            trial_features = []\n",
    "            for channel in range(trial.shape[1]):\n",
    "                channel_data = trial[:, channel]\n",
    "                coeffs = []\n",
    "                for lag in range(1, self.order + 1):\n",
    "                    # Shift the signal by the lag\n",
    "                    shifted = np.roll(channel_data, lag)\n",
    "                    shifted[:lag] = 0\n",
    "                    \n",
    "                    # Compute correlation between original and shifted\n",
    "                    slope, _, _, _, _ = linregress(shifted[lag:], channel_data[lag:])\n",
    "                    coeffs.append(slope)\n",
    "                trial_features.extend(coeffs)\n",
    "            features.append(trial_features)\n",
    "        return np.array(features)\n",
    "\n",
    "class ICAFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=4):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for trial in X:\n",
    "            # Transpose to (channels, time)\n",
    "            trial_data = trial.T\n",
    "            \n",
    "            # Apply ICA\n",
    "            ica = mne_ICA(n_components=self.n_components, method='fastica')\n",
    "            ica.fit(trial_data[np.newaxis, :, :])\n",
    "            \n",
    "            # Get the components\n",
    "            components = ica.get_components().T  # (n_components, channels)\n",
    "            \n",
    "            # Flatten components as features\n",
    "            features.append(components.flatten())\n",
    "        return np.array(features)\n",
    "\n",
    "def higuchi_fd(x, kmax=10):\n",
    "    \"\"\"Compute Higuchi Fractal Dimension of a time series\"\"\"\n",
    "    n = len(x)\n",
    "    lk = np.zeros(kmax)\n",
    "    x = np.asarray(x)\n",
    "    \n",
    "    for k in range(1, kmax+1):\n",
    "        lm = np.zeros((k,))\n",
    "        for m in range(k):\n",
    "            ll = 0\n",
    "            max_i = int(np.floor((n - m - 1) / k))\n",
    "            for i in range(1, max_i):\n",
    "                ll += abs(x[m + i*k] - x[m + (i-1)*k])\n",
    "            ll = ll * (n - 1) / (max_i * k)\n",
    "            lm[m] = np.log(ll / k)\n",
    "        lk[k-1] = np.mean(lm)\n",
    "    \n",
    "    hfd = np.polyfit(np.log(range(1, kmax+1)), lk, 1)[0]\n",
    "    return hfd\n",
    "\n",
    "class HiguchiFDFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, kmax=10):\n",
    "        self.kmax = kmax\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for trial in X:\n",
    "            trial_features = []\n",
    "            for channel in range(trial.shape[1]):\n",
    "                hfd = higuchi_fd(trial[:, channel], self.kmax)\n",
    "                trial_features.append(hfd)\n",
    "            features.append(trial_features)\n",
    "        return np.array(features)\n",
    "\n",
    "class FBCSPFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=4, freq_bands=None):\n",
    "      \n",
    "        self.n_components = n_components\n",
    "        if freq_bands is None:\n",
    "            # Default frequency bands for motor imagery\n",
    "            self.freq_bands = [(8, 12), (12, 16), (16, 24), (24, 30)]\n",
    "        else:\n",
    "            self.freq_bands = freq_bands\n",
    "        self.csp_models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize CSP for each frequency band\n",
    "        self.csp_models = []\n",
    "        for low, high in self.freq_bands:\n",
    "            # Bandpass filter the data\n",
    "            filtered = self._bandpass_filter(X, low, high)\n",
    "            \n",
    "            # Create and fit CSP\n",
    "            csp = CSP(n_components=self.n_components, reg=None, log=True)\n",
    "            csp.fit(filtered.transpose(0, 2, 1), y)  # MNE expects (trials, channels, time)\n",
    "            self.csp_models.append((low, high, csp))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for low, high, csp in self.csp_models:\n",
    "            # Filter and extract CSP features\n",
    "            filtered = self._bandpass_filter(X, low, high)\n",
    "            csp_feats = csp.transform(filtered.transpose(0, 2, 1))\n",
    "            features.append(csp_feats)\n",
    "            \n",
    "        # Concatenate features from all bands\n",
    "        return np.concatenate(features, axis=1)\n",
    "    \n",
    "    def _bandpass_filter(self, X, low, high):\n",
    "        nyquist = 0.5 * SAMPLING_RATE\n",
    "        low_norm = low / nyquist\n",
    "        high_norm = high / nyquist\n",
    "        b, a = butter(5, [low_norm, high_norm], btype='band')\n",
    "        \n",
    "        filtered = np.zeros_like(X)\n",
    "        for i in range(X.shape[0]):  # Filter each trial\n",
    "            for j in range(X.shape[2]):  # Filter each channel\n",
    "                filtered[i, :, j] = filtfilt(b, a, X[i, :, j])\n",
    "                \n",
    "        return filtered\n",
    "\n",
    "class FBRFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, freq_bands=None, metric='riemann'):\n",
    "        if freq_bands is None:\n",
    "            # Default frequency bands for motor imagery\n",
    "            self.freq_bands = [(8, 12), (12, 16), (16, 24), (24, 30)]\n",
    "        else:\n",
    "            self.freq_bands = freq_bands\n",
    "        self.metric = metric\n",
    "        self.cov_estimators = []\n",
    "        self.ts_transformers = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize covariances and tangent space for each frequency band\n",
    "        self.cov_estimators = []\n",
    "        self.ts_transformers = []\n",
    "        \n",
    "        for low, high in self.freq_bands:\n",
    "            # Bandpass filter the data\n",
    "            filtered = self._bandpass_filter(X, low, high)\n",
    "            \n",
    "            # Compute covariance matrices\n",
    "            cov = Covariances(estimator='lwf').fit(filtered.transpose(0, 2, 1))\n",
    "            \n",
    "            # Fit tangent space mapping\n",
    "            ts = TangentSpace(metric=self.metric).fit(cov.transform(filtered.transpose(0, 2, 1)))\n",
    "            \n",
    "            self.cov_estimators.append((low, high, cov))\n",
    "            self.ts_transformers.append((low, high, ts))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for (low, high, cov), (_, _, ts) in zip(self.cov_estimators, self.ts_transformers):\n",
    "            # Filter, compute covariances, and project to tangent space\n",
    "            filtered = self._bandpass_filter(X, low, high)\n",
    "            covs = cov.transform(filtered.transpose(0, 2, 1))\n",
    "            ts_feats = ts.transform(covs)\n",
    "            features.append(ts_feats)\n",
    "            \n",
    "        # Concatenate features from all bands\n",
    "        return np.concatenate(features, axis=1)\n",
    "    \n",
    "    def _bandpass_filter(self, X, low, high):\n",
    "      \n",
    "        nyquist = 0.5 * SAMPLING_RATE\n",
    "        low_norm = low / nyquist\n",
    "        high_norm = high / nyquist\n",
    "        b, a = butter(5, [low_norm, high_norm], btype='band')\n",
    "        \n",
    "        filtered = np.zeros_like(X)\n",
    "        for i in range(X.shape[0]):  # Filter each trial\n",
    "            for j in range(X.shape[2]):  # Filter each channel\n",
    "                filtered[i, :, j] = filtfilt(b, a, X[i, :, j])\n",
    "                \n",
    "        return filtered\n",
    "    \n",
    "class MatchingPursuit(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_atoms=20, max_iter=100, epsilon=0.1):\n",
    "        self.n_atoms = n_atoms\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.dictionary_ = None\n",
    "        \n",
    "    def _create_dictionary(self, signal_length):\n",
    "        \"\"\"Create a dictionary of Gabor atoms\"\"\"\n",
    "        t = np.linspace(0, 1, signal_length)\n",
    "        dictionary = []\n",
    "        \n",
    "        # Create Gabor atoms (windowed sinusoids)\n",
    "        for scale in np.linspace(0.1, 1.0, 5):\n",
    "            for freq in np.linspace(1, 30, 10):  # EEG relevant frequencies\n",
    "                for position in np.linspace(0, signal_length-1, 10, dtype=int):\n",
    "                    atom = gausspulse(t - t[position], fc=freq, bw=scale)\n",
    "                    atom /= np.linalg.norm(atom)  # Normalize\n",
    "                    dictionary.append(atom)\n",
    "        \n",
    "        # Add some chirp atoms\n",
    "        for f0 in [1, 5, 10]:\n",
    "            for f1 in [20, 30, 40]:\n",
    "                atom = chirp(t, f0=f0, f1=f1, t1=1, method='linear')\n",
    "                atom /= np.linalg.norm(atom)\n",
    "                dictionary.append(atom)\n",
    "                \n",
    "        self.dictionary_ = np.array(dictionary)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learn the dictionary based on input signal length\"\"\"\n",
    "        signal_length = X.shape[1] if len(X.shape) > 1 else len(X)\n",
    "        self._create_dictionary(signal_length)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Decompose signals using matching pursuit\"\"\"\n",
    "        if self.dictionary_ is None:\n",
    "            self.fit(X)\n",
    "            \n",
    "        results = []\n",
    "        for signal in X:\n",
    "            residual = signal.copy()\n",
    "            coefficients = np.zeros(len(self.dictionary_))\n",
    "            selected_atoms = []\n",
    "            \n",
    "            for _ in range(self.n_atoms):\n",
    "                if np.linalg.norm(residual) < self.epsilon:\n",
    "                    break\n",
    "                    \n",
    "                # Find atom with maximum correlation\n",
    "                correlations = np.abs(np.dot(self.dictionary_, residual))\n",
    "                best_idx = np.argmax(correlations)\n",
    "                best_atom = self.dictionary_[best_idx]\n",
    "                \n",
    "                # Update coefficients and residual\n",
    "                coeff = np.dot(best_atom, residual)\n",
    "                coefficients[best_idx] += coeff\n",
    "                residual -= coeff * best_atom\n",
    "                \n",
    "                selected_atoms.append(best_idx)\n",
    "            \n",
    "            # Extract features from the decomposition\n",
    "            if len(selected_atoms) > 0:\n",
    "                top_atoms = selected_atoms[:5]  # Take top 5 atoms\n",
    "                atom_features = []\n",
    "                for idx in top_atoms:\n",
    "                    atom = self.dictionary_[idx]\n",
    "                    # Features from each atom: position, frequency, scale, coefficient\n",
    "                    center = np.argmax(np.abs(atom))\n",
    "                    freq = np.argmax(np.abs(np.fft.fft(atom)[:len(atom)//2]))\n",
    "                    scale = np.std(atom)\n",
    "                    coeff = coefficients[idx]\n",
    "                    atom_features.extend([center, freq, scale, coeff])\n",
    "                \n",
    "                # Pad with zeros if not enough atoms were found\n",
    "                if len(atom_features) < 20:  # 5 atoms * 4 features\n",
    "                    atom_features.extend([0]*(20 - len(atom_features)))\n",
    "                \n",
    "                results.append(atom_features[:20])  # Return fixed-size feature vector\n",
    "            else:\n",
    "                results.append(np.zeros(20))\n",
    "        \n",
    "        return np.array(results)\n",
    "# ============================\n",
    "# Step 4: Enhanced Deep Learning Models \n",
    "# ============================\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 1) Define your PyTorch deep‐models\n",
    "# ------------------------\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, channels, samples, num_classes):\n",
    "        super().__init__()\n",
    "        # minimal EEGNet skeleton – adapt kernel sizes & paddings to your data\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 64), padding=(0, 32))\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.depthwise = nn.Conv2d(16, 32, kernel_size=(channels, 1), groups=16)\n",
    "        self.bn2       = nn.BatchNorm2d(32)\n",
    "        self.pool       = nn.AvgPool2d(kernel_size=(1, 4))\n",
    "        self.dropout    = nn.Dropout(0.25)\n",
    "        # classifier\n",
    "        self.classifier = nn.Linear(32 * ((samples // 4)), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, channels, samples)\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.depthwise(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self, channels, samples, num_classes):\n",
    "        super().__init__()\n",
    "        # minimal DeepConvNet skeleton\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(1, 25, (1, 5)), \n",
    "            nn.Conv2d(25, 25, (channels, 1)),\n",
    "            nn.BatchNorm2d(25), \n",
    "            nn.ELU(), \n",
    "            nn.MaxPool2d((1, 2)), \n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(25, 50, (1, 5)), \n",
    "            nn.BatchNorm2d(50), \n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)), \n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(50, 100, (1, 5)), \n",
    "            nn.BatchNorm2d(100), \n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)), \n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(100, 200, (1, 5)), \n",
    "            nn.BatchNorm2d(200), \n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)), \n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Calculate the correct output size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, channels, samples)\n",
    "            dummy_output = self.conv_blocks(dummy_input)\n",
    "            out_feat = dummy_output.view(1, -1).shape[1]\n",
    "            \n",
    "        self.classifier = nn.Linear(out_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional, dropout=0.3)\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.classifier = nn.Linear(hidden_size * factor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        # take last time‐step\n",
    "        out = out[:, -1, :]\n",
    "        return self.classifier(out)\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, n_channels=8, n_classes=2, n_samples=1000, dropout_rate=0.5):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(1, 16), padding=(0, 8), padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            \n",
    "            nn.Conv2d(32, 32, kernel_size=(n_channels, 1), groups=32),\n",
    "            nn.Conv2d(32, 64, kernel_size=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=(1, 8), padding=(0, 4)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 4)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.AvgPool2d(kernel_size=(1, 2)),\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if 2 > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, n_classes))\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='elu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_normal_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "                        if len(param) > 1:\n",
    "                            param.data[1::4].fill_(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        x = self.cnn(x)\n",
    "        x = x.squeeze(2).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "# ------------------------\n",
    "# 2) The unified trainer\n",
    "# ------------------------\n",
    "class ModelTrainer:\n",
    "    def __init__(self,\n",
    "                 model_type: str,\n",
    "                 input_shape,\n",
    "                 num_classes: int,\n",
    "                 device: str = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        model_type: one of ['LDA','SVM','RF','EEGNet','DeepConvNet','BiLSTM']\n",
    "        input_shape: for traditional: (n_features,). For EEGNet/DeepConvNet: (channels, samples).\n",
    "                     For BiLSTM: (seq_len, feature_dim).\n",
    "        num_classes: number of classes.\n",
    "        kwargs: extra hyperparams for each model type.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # device for pytorch\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # init model\n",
    "        if model_type == 'LDA':\n",
    "            self.model = LinearDiscriminantAnalysis(**kwargs)\n",
    "        elif model_type == 'SVM':\n",
    "            self.model = SVC(probability=True, **kwargs)\n",
    "        elif model_type == 'RF':\n",
    "            self.model = RandomForestClassifier(**kwargs)\n",
    "        elif model_type == 'EEGNet':\n",
    "            ch, samp = input_shape\n",
    "            self.model = EEGNet(ch, samp, num_classes).to(self.device)\n",
    "        elif model_type == 'DeepConvNet':\n",
    "            ch, samp = input_shape\n",
    "            self.model = DeepConvNet(ch, samp, num_classes).to(self.device)\n",
    "        elif model_type == 'BiLSTM':\n",
    "            seq_len, feat = input_shape\n",
    "            hidden = kwargs.get('hidden_size', 64)\n",
    "            layers = kwargs.get('num_layers', 1)\n",
    "            self.model = BiLSTM(feat, hidden, layers, num_classes).to(self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "        # only used for sklearn models\n",
    "        self.le = LabelEncoder() if model_type in ('LDA','SVM','RF') else None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, **train_kwargs):\n",
    "        \"\"\"\n",
    "        X_train, X_val: numpy arrays or PyTorch tensors\n",
    "        y_train, y_val: labels\n",
    "        \"\"\"\n",
    "        if self.model_type in ('LDA','SVM','RF'):\n",
    "            # encode labels\n",
    "            y_enc = self.le.fit_transform(y_train)\n",
    "            self.model.fit(X_train, y_enc)\n",
    "            if y_val is not None:\n",
    "                val_pred = self.model.predict(X_val)\n",
    "                acc = accuracy_score(self.le.transform(y_val), val_pred)\n",
    "                print(f\"Validation accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            # PyTorch training loop\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=train_kwargs.get('lr', 1e-3))\n",
    "            scheduler = train_kwargs.get('scheduler', None)\n",
    "\n",
    "            # build dataloaders\n",
    "            bs = train_kwargs.get('batch_size', 32)\n",
    "            train_ds = torch.utils.data.TensorDataset(\n",
    "                torch.tensor(X_train, dtype=torch.float32),\n",
    "                torch.tensor(y_train, dtype=torch.long))\n",
    "            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "            val_loader = None\n",
    "            if X_val is not None:\n",
    "                val_ds = torch.utils.data.TensorDataset(\n",
    "                    torch.tensor(X_val, dtype=torch.float32),\n",
    "                    torch.tensor(y_val, dtype=torch.long))\n",
    "                val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False)\n",
    "\n",
    "            # train\n",
    "            best_acc = 0\n",
    "            for epoch in range(train_kwargs.get('epochs', 20)):\n",
    "                self.model.train()\n",
    "                for xb, yb in train_loader:\n",
    "                    xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = self.model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if val_loader:\n",
    "                    self.model.eval()\n",
    "                    all_preds, all_labels = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for xb, yb in val_loader:\n",
    "                            xb = xb.to(self.device)\n",
    "                            logits = self.model(xb)\n",
    "                            preds = logits.argmax(1).cpu().numpy()\n",
    "                            all_preds += preds.tolist()\n",
    "                            all_labels += yb.numpy().tolist()\n",
    "                    acc = accuracy_score(all_labels, all_preds)\n",
    "                    print(f\"[{self.model_type}] Epoch {epoch+1} val acc: {acc:.4f}\")\n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        torch.save(self.model.state_dict(), f\"best_{self.model_type}.pt\")\n",
    "            # restore best\n",
    "            if val_loader:\n",
    "                self.model.load_state_dict(torch.load(f\"best_{self.model_type}.pt\"))\n",
    "                print(f\"Best val acc for {self.model_type}: {best_acc:.4f}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model_type in ('LDA','SVM','RF'):\n",
    "            proba = self.model.predict_proba(X_test)\n",
    "            preds = self.le.inverse_transform(proba.argmax(1))\n",
    "            return preds, proba\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                X = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "                logits = self.model(X)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "            return preds, probs\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# assume EEGPreprocessor, CSPFeatures, STFTFeatures, RawEEGFeatures,\n",
    "# and ModelTrainer are already imported\n",
    "\n",
    "def load_raw_eeg(df, base_path):\n",
    "    raws = []\n",
    "    for idx, row in df.iterrows():\n",
    "        task = row['task']\n",
    "        subject = row['subject_id']\n",
    "        session = row['trial_session']\n",
    "        fpath = os.path.join(base_path, task, 'train' if row['id'] <= 4800 else 'validation' if row['id'] <= 4900 else 'test', subject, str(session), 'EEGdata.csv')\n",
    "        eeg = pd.read_csv(fpath)\n",
    "\n",
    "        # Extract correct trial slice\n",
    "        trial = int(row['trial'])\n",
    "        if task == 'MI':\n",
    "            samples_per_trial = 2250\n",
    "        else:\n",
    "            samples_per_trial = 1750\n",
    "        start = (trial - 1) * samples_per_trial\n",
    "        end = start + samples_per_trial\n",
    "        raws.append(eeg.iloc[start:end])\n",
    "    return raws\n",
    "\n",
    "\n",
    "def main_MI(model_type='LDA',\n",
    "            feature_type='CSP',\n",
    "            n_components=4,\n",
    "            k_best=100,\n",
    "            trainer_kwargs=None,\n",
    "            order=5,\n",
    "            kmax=10):\n",
    "    \"\"\"\n",
    "    model_type: 'LDA','SVM','RF','EEGNet','DeepConvNet','BiLSTM'\n",
    "    feature_type: 'CSP','STFT','RAW'\n",
    "    \"\"\"\n",
    "    trainer_kwargs = trainer_kwargs or {}\n",
    "    base_path = '/kaggle/input/mtcaic3'\n",
    "\n",
    "    # 1) Load index CSVs + label encoders\n",
    "    print(\"Loading index files...\")\n",
    "    train_df, val_df, test_df, le_mi = load_index_csvs_MI(base_path)\n",
    "    # Keep only MI rows\n",
    "    train_df = train_df[train_df['task'] == 'MI'].copy()\n",
    "    val_df = val_df[val_df['task'] == 'MI'].copy()\n",
    "    test_df = test_df[test_df['task'] == 'MI'].copy()\n",
    "\n",
    "    # Ensure labels are integers\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "    val_df['label'] = val_df['label'].astype(int)\n",
    "\n",
    "    num_classes = len(le_mi.classes_)\n",
    "    print(f\"  Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "\n",
    "    # 2) Read raw EEG data\n",
    "    X_tr_raw = load_raw_eeg(train_df, base_path); y_tr = train_df.label.values\n",
    "    X_val_raw = load_raw_eeg(val_df, base_path);   y_val = val_df.label.values\n",
    "    X_te_raw = load_raw_eeg(test_df, base_path)\n",
    "\n",
    "    # 3) Preprocess → epochs\n",
    "    preproc = EEGPreprocessor()\n",
    "    X_tr_ep = preproc.fit_transform(X_tr_raw)\n",
    "    X_val_ep = preproc.transform(X_val_raw)\n",
    "    X_te_ep = preproc.transform(X_te_raw)\n",
    "    print(f\"  Epochs: train={len(X_tr_ep)}, val={len(X_val_ep)}, test={len(X_te_ep)}\")\n",
    "\n",
    "    # 4) Feature extraction\n",
    "    if feature_type == 'CSP':\n",
    "        feat_ext = CSPFeatures(n_components=n_components)\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep, y_tr)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "\n",
    "    elif feature_type == 'STFT':\n",
    "        feat_ext = STFTFeatures()\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "\n",
    "    elif feature_type == 'RAW':\n",
    "        feat_ext = RawEEGFeatures()\n",
    "        X_tr_ft = feat_ext.transform(X_tr_ep)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "        \n",
    "    elif feature_type == 'AutoRegressionFeatures':\n",
    "        feat_ext = AutoRegressionFeatures(order=order)\n",
    "        X_tr_ft = feat_ext.transform(X_tr_ep)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "\n",
    "    elif feature_type =='HiguchiFDFeatures':\n",
    "        feat_ext = HiguchiFDFeatures(kmax=kmax)\n",
    "        X_tr_ft = feat_ext.transform(X_tr_ep)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "\n",
    "    elif feature_type =='ICAFeatures':\n",
    "        feat_ext = CSPFeatures(n_components=n_components)\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep, y_tr)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "    elif feature_type == 'FBCSP':  # Add this option\n",
    "        feat_ext = FBCSPFeatures(n_components=n_components)\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep, y_tr)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "\n",
    "    elif feature_type == 'FBR':  # Add this option\n",
    "        feat_ext = FBRFeatures()\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep, y_tr)\n",
    "        X_val_ft = feat_ext.transform(X_val_ep)\n",
    "        X_te_ft = feat_ext.transform(X_te_ep)\n",
    "    # In the feature extraction section of main_MI():\n",
    "    elif feature_type == 'MatchingPursuit':\n",
    "        feat_ext = MatchingPursuit(n_atoms=20)\n",
    "        X_tr_ft = feat_ext.fit_transform(X_tr_ep.reshape(X_tr_ep.shape[0], -1))  # Flatten time×channels\n",
    "        X_val_ft = feat_ext.transform(X_val_ep.reshape(X_val_ep.shape[0], -1))\n",
    "        X_te_ft = feat_ext.transform(X_te_ep.reshape(X_te_ep.shape[0], -1))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"feature_type must be one of 'CSP','STFT','RAW'\")\n",
    "\n",
    "    # 5) Scale + SelectKBest (only for sklearn models)\n",
    "    if model_type in ('LDA','SVM','RF'):\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_sc = scaler.fit_transform(X_tr_ft)\n",
    "        X_val_sc = scaler.transform(X_val_ft)\n",
    "        X_te_sc = scaler.transform(X_te_ft)\n",
    "\n",
    "        k = min(k_best, X_tr_sc.shape[1])\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "        X_tr_sel = selector.fit_transform(X_tr_sc, y_tr)\n",
    "        X_val_sel = selector.transform(X_val_sc)\n",
    "        X_te_sel = selector.transform(X_te_sc)\n",
    "        print(f\"  Selected features: {X_tr_sel.shape[1]} / {X_tr_sc.shape[1]}\")\n",
    "\n",
    "        # 6a) Train sklearn model\n",
    "        trainer = ModelTrainer(model_type,\n",
    "                               input_shape=(X_tr_sel.shape[1],),\n",
    "                               num_classes=num_classes)\n",
    "        trainer.fit(X_tr_sel, y_tr, X_val_sel, y_val)\n",
    "        trainer.fit(X_tr_sel, y_tr, X_val_sel, y_val)\n",
    "        preds, probs = trainer.predict(X_te_sel)\n",
    "\n",
    "    \n",
    "        out_df = pd.DataFrame({\n",
    "            'id': test_df.id.values,\n",
    "            'label': preds,\n",
    "            'confidence': probs.max(axis=1)\n",
    "        })\n",
    "\n",
    "    # 6b) Train deep‐learning model\n",
    "    else:\n",
    "        # shape for PyTorch nets\n",
    "        if model_type in ('EEGNet','DeepConvNet'):\n",
    "            # expects (batch,1,channels,time)\n",
    "            # X_tr_ep: (n_trials, time, channels) → swap axes\n",
    "            X_tr_in = X_tr_ep.transpose(0,2,1)[:, None, :, :]\n",
    "            X_val_in = X_val_ep.transpose(0,2,1)[:, None, :, :]\n",
    "            X_te_in = X_te_ep.transpose(0,2,1)[:, None, :, :]\n",
    "            input_shape = (X_tr_ep.shape[2], X_tr_ep.shape[1])\n",
    "        elif model_type == 'BiLSTM':\n",
    "            # expects (batch, time, features)\n",
    "            X_tr_in = X_tr_ft if feature_type!='RAW' else X_tr_ep.reshape(len(X_tr_ep), -1)\n",
    "            X_val_in = X_val_ft if feature_type!='RAW' else X_val_ep.reshape(len(X_val_ep), -1)\n",
    "            X_te_in = X_te_ft if feature_type!='RAW' else X_te_ep.reshape(len(X_te_ep), -1)\n",
    "            input_shape = X_tr_in.shape[1:]  # (seq_len, feat_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown deep model type\")\n",
    "\n",
    "        # build trainer and dataloaders\n",
    "        trainer = ModelTrainer(model_type,\n",
    "                               input_shape=input_shape,\n",
    "                               num_classes=num_classes,\n",
    "                               **trainer_kwargs)\n",
    "\n",
    "        # .fit expects numpy arrays\n",
    "        trainer.fit(X_tr_in, y_tr, X_val_in, y_val)\n",
    "        preds, probs = trainer.predict(X_te_in)\n",
    "\n",
    "        out_df = pd.DataFrame({\n",
    "            'id': test_df.id.values,\n",
    "            'label': preds,\n",
    "            'confidence': probs.max(axis=1)\n",
    "        })\n",
    "\n",
    "    # 7) save & sanity‐check\n",
    "    out_df.to_csv('vs_submission.csv', index=False)\n",
    "    print(\"✅ Saved submission.csv\")\n",
    "    missing = set(test_df.id) - set(out_df.id)\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing IDs: {missing}\")\n",
    "    else:\n",
    "        print(\"✓ All test IDs covered\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example using FBCSP with LDA\n",
    "    main_MI(model_type='EEGNet', feature_type='FBCSP',\n",
    "            n_components=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12673416,
     "sourceId": 98188,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
