{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T22:12:47.402704Z",
     "iopub.status.busy": "2025-06-28T22:12:47.402423Z",
     "iopub.status.idle": "2025-06-28T22:19:17.731341Z",
     "shell.execute_reply": "2025-06-28T22:19:17.730562Z",
     "shell.execute_reply.started": "2025-06-28T22:12:47.402684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index files...\n",
      "Train samples: 4800\n",
      "Validation samples: 100\n",
      "Test samples: 100\n",
      "\n",
      "Task distribution:\n",
      "Training: task\n",
      "MI       2400\n",
      "SSVEP    2400\n",
      "dtype: int64\n",
      "Validation: task\n",
      "MI       50\n",
      "SSVEP    50\n",
      "dtype: int64\n",
      "Test: task\n",
      "MI       50\n",
      "SSVEP    50\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "Processing SSVEP Task\n",
      "==================================================\n",
      "Loading SSVEP training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for SSVEP: 100%|██████████| 2400/2400 [05:41<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2400 samples with 4 features\n",
      "Loading SSVEP validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for SSVEP: 100%|██████████| 50/50 [00:06<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 50 samples with 4 features\n",
      "Loading SSVEP test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for SSVEP: 100%|██████████| 50/50 [00:06<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 50 samples with 4 features\n",
      "Scaling features...\n",
      "Selecting best features...\n",
      "Selected 4 features out of 4\n",
      "Model architecture for SSVEP:\n",
      "  Input dimension: 4\n",
      "  Number of classes: 4\n",
      "  Model parameters: 701,508\n",
      "\n",
      "Training SSVEP model...\n",
      "Training SSVEP model on device: cpu\n",
      "Epoch 5/60\n",
      "  Train Loss: 1.0362, Train Acc: 0.5783\n",
      "  Val Loss: 1.3054, Val Acc: 0.3600\n",
      "  LR: 0.000320\n",
      "Epoch 10/60\n",
      "  Train Loss: 1.0022, Train Acc: 0.5863\n",
      "  Val Loss: 1.3026, Val Acc: 0.3800\n",
      "  LR: 0.000640\n",
      "Epoch 15/60\n",
      "  Train Loss: 1.0151, Train Acc: 0.5962\n",
      "  Val Loss: 1.3025, Val Acc: 0.4400\n",
      "  LR: 0.000320\n",
      "Epoch 20/60\n",
      "  Train Loss: 0.9759, Train Acc: 0.6104\n",
      "  Val Loss: 1.3238, Val Acc: 0.3800\n",
      "  LR: 0.000640\n",
      "Epoch 25/60\n",
      "  Train Loss: 0.9940, Train Acc: 0.6017\n",
      "  Val Loss: 1.3358, Val Acc: 0.3800\n",
      "  LR: 0.000320\n",
      "Early stopping at epoch 29\n",
      "Best validation accuracy: 0.4600\n",
      "Making predictions for SSVEP...\n",
      "SSVEP predictions completed. Sample predictions:\n",
      "     id     label  confidence   task\n",
      "0  4951  Backward    0.925140  SSVEP\n",
      "1  4952  Backward    0.932307  SSVEP\n",
      "2  4953   Forward    0.870471  SSVEP\n",
      "3  4954  Backward    0.934347  SSVEP\n",
      "4  4955  Backward    0.923205  SSVEP\n",
      "Confidence stats: mean=0.657, std=0.193\n",
      "\n",
      "==================================================\n",
      "Combining Predictions\n",
      "==================================================\n",
      "Submission file saved as: submission.csv\n",
      "Total predictions: 50\n",
      "\n",
      "Final prediction distribution:\n",
      "task   label   \n",
      "SSVEP  Backward    13\n",
      "       Forward     11\n",
      "       Left        22\n",
      "       Right        4\n",
      "dtype: int64\n",
      "\n",
      "Submission preview:\n",
      "     id     label\n",
      "0  4951  Backward\n",
      "1  4952  Backward\n",
      "2  4953   Forward\n",
      "3  4954  Backward\n",
      "4  4955  Backward\n",
      "5  4956      Left\n",
      "6  4957      Left\n",
      "7  4958   Forward\n",
      "8  4959   Forward\n",
      "9  4960  Backward\n",
      "...\n",
      "      id     label\n",
      "40  4991   Forward\n",
      "41  4992      Left\n",
      "42  4993      Left\n",
      "43  4994  Backward\n",
      "44  4995      Left\n",
      "45  4996  Backward\n",
      "46  4997   Forward\n",
      "47  4998     Right\n",
      "48  4999  Backward\n",
      "49  5000   Forward\n",
      "⚠ Missing predictions for IDs: {4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "import torch\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================\n",
    "# Step 1: Data Loading with Preprocessing (SSVEP only)\n",
    "# ============================\n",
    "def load_index_csvs(base_path):\n",
    "    train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
    "    validation_df = pd.read_csv(os.path.join(base_path, 'validation.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
    "\n",
    "    le_ssvep = LabelEncoder()\n",
    "\n",
    "    ssvep_train_labels = train_df[train_df['task'] == 'SSVEP']['label']\n",
    "    if len(ssvep_train_labels) > 0:\n",
    "        le_ssvep.fit(ssvep_train_labels)\n",
    "\n",
    "        for df in [train_df, validation_df]:\n",
    "            if 'label' in df.columns:\n",
    "                ssvep_mask = df['task'] == 'SSVEP'\n",
    "                if ssvep_mask.any():\n",
    "                    df.loc[ssvep_mask, 'label'] = le_ssvep.transform(df.loc[ssvep_mask, 'label'])\n",
    "\n",
    "    return train_df, validation_df, test_df, le_ssvep\n",
    "\n",
    "\n",
    "# ============================\n",
    "# FBCCA Feature Extractor\n",
    "# ============================\n",
    "\n",
    "class FBCCAExtractor:\n",
    "    def __init__(self, fs=250, num_harmonics=2, num_subbands=5):\n",
    "        self.fs = fs\n",
    "        self.num_harmonics = num_harmonics\n",
    "        self.num_subbands = num_subbands\n",
    "        self.target_freqs = [7, 8, 10, 13]  # SSVEP targets\n",
    "        self.subbands = [\n",
    "            (5, 40), (6, 38), (7, 36), (8, 34), (9, 32)\n",
    "        ][:num_subbands]\n",
    "\n",
    "    def _bandpass_filter(self, data, low_freq, high_freq, order=4):\n",
    "        nyq = 0.5 * self.fs\n",
    "        low = low_freq / nyq\n",
    "        high = high_freq / nyq\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "    def _generate_reference_signals(self, freq, n_samples):\n",
    "        t = np.arange(n_samples) / self.fs\n",
    "        ref = [\n",
    "            np.sin(2 * np.pi * freq * i * t) for i in range(1, self.num_harmonics+1)\n",
    "        ] + [\n",
    "            np.cos(2 * np.pi * freq * i * t) for i in range(1, self.num_harmonics+1)\n",
    "        ]\n",
    "        return np.stack(ref, axis=1)\n",
    "\n",
    "    def _cca_correlation(self, X, Y):\n",
    "        cca = CCA(n_components=1)\n",
    "        cca.fit(X, Y)\n",
    "        X_c, Y_c = cca.transform(X, Y)\n",
    "        return np.corrcoef(X_c.T, Y_c.T)[0, 1]\n",
    "\n",
    "    def extract_fbcca_features(self, eeg_data):\n",
    "        n_samples = eeg_data.shape[0]\n",
    "        corrs = []\n",
    "\n",
    "        for low, high in self.subbands:\n",
    "            filtered = self._bandpass_filter(eeg_data, low, high)\n",
    "            sub_corrs = [\n",
    "                self._cca_correlation(filtered, self._generate_reference_signals(freq, n_samples))\n",
    "                for freq in self.target_freqs\n",
    "            ]\n",
    "            corrs.append(sub_corrs)\n",
    "\n",
    "        corrs = np.array(corrs)\n",
    "        weights = 1 / np.arange(1, self.num_subbands + 1)\n",
    "        weights /= weights.sum()\n",
    "        return np.dot(weights, corrs)  # shape: (num_targets,)\n",
    "\n",
    "# ============================\n",
    "# Feature Extractor Using FBCCA Only\n",
    "# ============================\n",
    "class EEGFeatureExtractor:\n",
    "    def __init__(self, fs=250):\n",
    "        self.fs = fs\n",
    "        self.eeg_channels = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n",
    "        self.fbcca_extractor = FBCCAExtractor(fs=fs)\n",
    "\n",
    "    def extract_features(self, trial_data):\n",
    "        # تأكد إن كل القنوات موجودة\n",
    "        available_channels = [ch for ch in self.eeg_channels if ch in trial_data.columns]\n",
    "        if not available_channels:\n",
    "            raise ValueError(\"No valid EEG channels found in trial data\")\n",
    "\n",
    "        eeg_data = trial_data[available_channels].values\n",
    "\n",
    "        try:\n",
    "            fbcca_feats = self.fbcca_extractor.extract_fbcca_features(eeg_data)\n",
    "        except Exception as e:\n",
    "            print(f\"FBCCA failed: {e}\")\n",
    "            fbcca_feats = np.zeros(len(self.fbcca_extractor.target_freqs))\n",
    "\n",
    "        return fbcca_feats\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Data Loading with Features\n",
    "# ============================\n",
    "def load_trial_data_with_features(row, base_path, feature_extractor):\n",
    "    dataset = 'train' if row['id'] <= 4800 else 'validation' if row['id'] <= 4900 else 'test'\n",
    "    eeg_path = os.path.join(base_path, row['task'], dataset, str(row['subject_id']), str(row['trial_session']), 'EEGdata.csv')\n",
    "\n",
    "    if not os.path.exists(eeg_path):\n",
    "        raise FileNotFoundError(f\"File not found: {eeg_path}\")\n",
    "\n",
    "    eeg_data = pd.read_csv(eeg_path)\n",
    "    trial = int(row['trial'])\n",
    "    start = (trial - 1) * 1750\n",
    "    end = start + 1750\n",
    "    trial_data = eeg_data.iloc[start:end].copy()\n",
    "\n",
    "    features = feature_extractor.extract_features(trial_data)\n",
    "    result = {'id': row['id'], 'features': features, 'task': row['task']}\n",
    "    if 'label' in row and pd.notna(row['label']):\n",
    "        result['label'] = row['label']\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_all_split_data_with_features(split_df, base_path, task='SSVEP'):\n",
    "    all_trials = []\n",
    "    split_df = split_df[split_df['task'] == task]\n",
    "    extractor = EEGFeatureExtractor()\n",
    "\n",
    "    for _, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Extracting features for {task}\"):\n",
    "        try:\n",
    "            trial = load_trial_data_with_features(row, base_path, extractor)\n",
    "            all_trials.append(trial)\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Trial {row['id']} failed: {e}\")\n",
    "\n",
    "    if not all_trials:\n",
    "        return None, None, None\n",
    "\n",
    "    features = np.array([t['features'] for t in all_trials])\n",
    "    labels = np.array([t['label'] for t in all_trials if 'label' in t])\n",
    "    ids = np.array([t['id'] for t in all_trials])\n",
    "\n",
    "    print(f\"✓ Loaded {len(features)} samples with {features.shape[1]} features\")\n",
    "    return features, labels, ids\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Step 4: Deep Learning Models (SSVEP only)\n",
    "# ============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------------\n",
    "# Attention Module for Feature Enhancement\n",
    "# ----------------------------------\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim // 2, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.attention(x)\n",
    "        return x * weights\n",
    "\n",
    "# ----------------------------------\n",
    "# MLP-Based Classifier with Attention\n",
    "# ----------------------------------\n",
    "class EnhancedFeatureClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EnhancedFeatureClassifier, self).__init__()\n",
    "\n",
    "        hidden_dim1 = max(256, input_dim // 4)\n",
    "        hidden_dim2 = max(128, input_dim // 8)\n",
    "        hidden_dim3 = max(64, input_dim // 16)\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.BatchNorm1d(hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionModule(hidden_dim3)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim3 // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(hidden_dim3 // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.attention(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ----------------------------------\n",
    "# Transformer-Based SSVEP Classifier\n",
    "# ----------------------------------\n",
    "class SSVEPFormer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(SSVEPFormer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ----------------------------------\n",
    "# CNN-Based Classifier\n",
    "# ----------------------------------\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(16)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B, 1, input_dim)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ----------------------------------\n",
    "# EEGNet Architecture\n",
    "# ----------------------------------\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(F.relu(self.bn2(self.fc2(x))))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# ----------------------------------\n",
    "# BiLSTM-Based Classifier\n",
    "# ----------------------------------\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=128, num_layers=1):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B, seq_len=1, input_dim)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = torch.cat((h_n[-2], h_n[-1]), dim=1)  # Concatenate forward and backward\n",
    "        return self.fc(h)\n",
    "\n",
    "# ----------------------------------\n",
    "# Transformer + LSTM Hybrid Model\n",
    "# ----------------------------------\n",
    "class TransformerLSTMHybrid(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=4, num_layers=1, lstm_hidden=64):\n",
    "        super(TransformerLSTMHybrid, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        return self.classifier(out)\n",
    "\n",
    "# ============================\n",
    "# Step 5: Feature-based Dataset\n",
    "# ============================\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels=None, ids=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels) if labels is not None else None\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.features[idx], self.ids[idx] if self.ids is not None else idx\n",
    "\n",
    "# ============================\n",
    "# Step 6: Training for SSVEP\n",
    "# ============================\n",
    "def train_model_advanced(model, train_loader, val_loader, epochs=50, lr=1e-3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training SSVEP model on device: {device}\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr * 0.8, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "    max_patience = 15\n",
    "\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            if len(batch) == 2:\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            # L2 regularization\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "            loss += 0.0001 * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == y).sum().item()\n",
    "            total_train += y.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(batch) == 2:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == y).sum().item()\n",
    "                total_val += y.size(0)\n",
    "\n",
    "        train_acc = correct_train / total_train if total_train > 0 else 0\n",
    "        val_acc = correct_val / total_val if total_val > 0 else 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_SSVEP_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if os.path.exists('best_SSVEP_model.pth'):\n",
    "        model.load_state_dict(torch.load('best_SSVEP_model.pth'))\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def predict_model_advanced(model, test_loader, le=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ids = []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if len(batch) == 2:\n",
    "                x, id_batch = batch\n",
    "            else:\n",
    "                x = batch\n",
    "                id_batch = list(range(len(x)))\n",
    "\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "\n",
    "            processed_ids = []\n",
    "            for id_val in id_batch:\n",
    "                if torch.is_tensor(id_val):\n",
    "                    processed_ids.append(int(id_val.item()))\n",
    "                else:\n",
    "                    processed_ids.append(int(id_val))\n",
    "            ids.extend(processed_ids)\n",
    "\n",
    "    if le is not None:\n",
    "        predictions = le.inverse_transform(predictions)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'label': predictions,\n",
    "        'confidence': [max(prob) for prob in probabilities]\n",
    "    })\n",
    "\n",
    "# ============================\n",
    "# Step 7: Main Pipeline for SSVEP Only\n",
    "# ============================\n",
    "\n",
    "def get_model(model_name, input_dim, num_classes):\n",
    "    if model_name == \"EnhancedFeatureClassifier\":\n",
    "        return EnhancedFeatureClassifier(input_dim, num_classes)\n",
    "    elif model_name == \"SSVEPFormer\":\n",
    "        return SSVEPFormer(input_dim, num_classes)\n",
    "    elif model_name == \"CNNClassifier\":\n",
    "        return CNNClassifier(input_dim, num_classes)\n",
    "    elif model_name == \"EEGNet\":\n",
    "        return EEGNet(input_dim, num_classes)\n",
    "    elif model_name == \"BiLSTMClassifier\":\n",
    "        return BiLSTMClassifier(input_dim, num_classes)\n",
    "    elif model_name == \"TransformerLSTMHybrid\":\n",
    "        return TransformerLSTMHybrid(input_dim, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "def main():\n",
    "    base_path = '/kaggle/input/mtcaic3'  # Update this if needed\n",
    "\n",
    "    print(\"Loading index files...\")\n",
    "    train_df, val_df, test_df, le_ssvep = load_index_csvs(base_path)\n",
    "\n",
    "\n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "    print(\"\\nTask distribution:\")\n",
    "    print(\"Training:\", train_df.groupby('task').size())\n",
    "    print(\"Validation:\", val_df.groupby('task').size())\n",
    "    print(\"Test:\", test_df.groupby('task').size())\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    task = 'SSVEP'\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {task} Task\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    num_classes = 4\n",
    "    le = le_ssvep\n",
    "\n",
    "    print(f\"Loading {task} training data...\")\n",
    "    train_features, train_labels, train_ids = load_all_split_data_with_features(\n",
    "        train_df, base_path, task=task\n",
    "    )\n",
    "\n",
    "    print(f\"Loading {task} validation data...\")\n",
    "    val_features, val_labels, val_ids = load_all_split_data_with_features(\n",
    "        val_df, base_path, task=task\n",
    "    )\n",
    "\n",
    "    print(f\"Loading {task} test data...\")\n",
    "    test_features, _, test_ids = load_all_split_data_with_features(\n",
    "        test_df, base_path, task=task\n",
    "    )\n",
    "\n",
    "    if train_features is None or val_features is None or test_features is None:\n",
    "        print(f\"Skipping {task} due to data loading issues\")\n",
    "        return\n",
    "\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    val_features_scaled = scaler.transform(val_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "    print(\"Selecting best features...\")\n",
    "    selector = SelectKBest(f_classif, k=min(500, train_features_scaled.shape[1]))\n",
    "    train_features_selected = selector.fit_transform(train_features_scaled, train_labels)\n",
    "    val_features_selected = selector.transform(val_features_scaled)\n",
    "    test_features_selected = selector.transform(test_features_scaled)\n",
    "\n",
    "    print(f\"Selected {train_features_selected.shape[1]} features out of {train_features_scaled.shape[1]}\")\n",
    "\n",
    "    train_dataset = FeatureDataset(train_features_selected, train_labels)\n",
    "    val_dataset = FeatureDataset(val_features_selected, val_labels)\n",
    "    test_dataset = FeatureDataset(test_features_selected, ids=test_ids)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = train_features_selected.shape[1]\n",
    "    model_name = \"EnhancedFeatureClassifier\"  # ← غير الاسم لأي نموذج آخر لتجربته\n",
    "    model = get_model(model_name, input_dim, num_classes)\n",
    "   \n",
    "\n",
    "    print(f\"Model architecture for {task}:\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Number of classes: {num_classes}\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    print(f\"\\nTraining {task} model...\")\n",
    "    model = train_model_advanced(\n",
    "        model, train_loader, val_loader, epochs=60, lr=0.0008\n",
    "    )\n",
    "\n",
    "    print(f\"Making predictions for {task}...\")\n",
    "    task_predictions = predict_model_advanced(model, test_loader, le)\n",
    "    task_predictions['task'] = task\n",
    "    all_predictions.append(task_predictions)\n",
    "\n",
    "    print(f\"{task} predictions completed. Sample predictions:\")\n",
    "    print(task_predictions.head())\n",
    "    print(f\"Confidence stats: mean={task_predictions['confidence'].mean():.3f}, \"\n",
    "          f\"std={task_predictions['confidence'].std():.3f}\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Combining Predictions\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    final_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    final_predictions = final_predictions.sort_values('id')\n",
    "\n",
    "    submission = final_predictions[['id', 'label']].copy()\n",
    "    submission_path = 'submission.csv'\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"Submission file saved as: {submission_path}\")\n",
    "    print(f\"Total predictions: {len(submission)}\")\n",
    "    print(\"\\nFinal prediction distribution:\")\n",
    "    print(final_predictions.groupby(['task', 'label']).size())\n",
    "\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head(10))\n",
    "    print(\"...\")\n",
    "    print(submission.tail(10))\n",
    "\n",
    "    expected_test_ids = set(test_df['id'].values)\n",
    "    predicted_ids = set(submission['id'].values)\n",
    "\n",
    "    if expected_test_ids == predicted_ids:\n",
    "        print(\"✓ All test IDs have predictions\")\n",
    "    else:\n",
    "        missing_ids = expected_test_ids - predicted_ids\n",
    "        extra_ids = predicted_ids - expected_test_ids\n",
    "        if missing_ids:\n",
    "            print(f\"⚠ Missing predictions for IDs: {missing_ids}\")\n",
    "        if extra_ids:\n",
    "            print(f\"⚠ Extra predictions for IDs: {extra_ids}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12673416,
     "sourceId": 98188,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
